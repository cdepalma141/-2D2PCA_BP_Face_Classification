{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training_NN Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Notebook is used to build, train, and evaluate a simple Backpropagation NN\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 4, 68)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from preprocessing step\n",
    "features = np.load(\"./PP_Data/features.npy\")\n",
    "labels = np.load(\"./PP_Data/labels.npy\")\n",
    "\n",
    "test_features = np.load(\"./PP_Data/test_features.npy\")\n",
    "test_labels = np.load(\"./PP_Data/test_labels.npy\")\n",
    "\n",
    "n_input = features.shape[1] # number of input nodes in 1st layer\n",
    "n_class = labels.shape[1] # number of output nodes in last layer\n",
    "\n",
    "# batch size for training (right now it is the number of original training images\n",
    "# found by divide the current number of samples by the number of epochs used in new image generation)\n",
    "batch_size = features.shape[0]//10 \n",
    "n_input, n_class, batch_size # Check number of input features, classes, and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the base function used to create a Keras NN using sklearn's RandomizedCV optimization\n",
    "# based off of http://ethen8181.github.io/machine-learning/keras/nn_keras_hyperparameter_tuning.html\n",
    "def build_keras_base(hidden_layers = [50], dropout_rate = 0, \n",
    "                     l2_penalty = 0.1, optimizer = 'adam',\n",
    "                     n_input = n_input, n_class = n_class):\n",
    "    \"\"\"\n",
    "    Keras Multi-layer neural network. Fixed parameters include: \n",
    "    1. activation function (PRelu)\n",
    "    2. always uses batch normalization after the activation\n",
    "    3. use adam as the optimizer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Tunable parameters are (commonly tuned)\n",
    "    \n",
    "    hidden_layers: list\n",
    "        the number of hidden layers, and the size of each hidden layer\n",
    "    \n",
    "    dropout_rate: float 0 ~ 1\n",
    "        if bigger than 0, there will be a dropout layer\n",
    "    \n",
    "    l2_penalty: float\n",
    "        or so called l2 regularization\n",
    "    \n",
    "    optimizer: string or keras optimizer\n",
    "        method to train the network\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : \n",
    "        a keras model\n",
    "\n",
    "    Reference\n",
    "    ---------\n",
    "    https://keras.io/scikit-learn-api/\n",
    "    \"\"\"   \n",
    "    model = Sequential() # model type\n",
    "    # Dynamically make hidden layers based on parameters\n",
    "    for index, layers in enumerate(hidden_layers):       \n",
    "        if not index:\n",
    "            # specify the input_dim to be the number of features for the first layer\n",
    "            model.add(Dense(layers, input_dim = n_input, kernel_regularizer = l2(l2_penalty)))\n",
    "        else:\n",
    "            model.add(Dense(layers, kernel_regularizer = l2(l2_penalty)))\n",
    "        \n",
    "        # insert BatchNorm layer immediately after fully connected layers\n",
    "        # and before activation layer\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(PReLU())        \n",
    "        if dropout_rate:\n",
    "            model.add(Dropout(p = dropout_rate))\n",
    "    \n",
    "    model.add(Dense(n_class))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # the loss for binary and muti-class classification is different \n",
    "    loss = 'binary_crossentropy'\n",
    "    if n_class > 2:\n",
    "        loss = 'categorical_crossentropy'\n",
    "    \n",
    "    model.compile(loss = loss, optimizer = optimizer, metrics = ['accuracy'])   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in fixed parameters n_input and n_class along with the previous function for cross-validation\n",
    "model_keras = KerasClassifier(\n",
    "    build_fn = build_keras_base,\n",
    "    n_input = n_input,\n",
    "    n_class = n_class,\n",
    ")\n",
    "\n",
    "# The following is also based on http://ethen8181.github.io/machine-learning/keras/nn_keras_hyperparameter_tuning.html\n",
    "# random search's parameter:\n",
    "# specify the options and store them inside the dictionary\n",
    "# batch size and training method can also be hyperparameters, \n",
    "# but it is fixed\n",
    "early_stop = EarlyStopping(\n",
    "    monitor = 'val_loss', min_delta = 0.1, patience = 5, verbose = 0)\n",
    "\n",
    "# Callbacks can be used when fitting the data to allow the process to stop when certain criteria is met\n",
    "callbacks = [early_stop]\n",
    "\n",
    "# Set the parameters for fitting the model\n",
    "keras_fit_params = {   \n",
    "    'callbacks': callbacks,\n",
    "    'epochs': 100,\n",
    "    'batch_size': batch_size,\n",
    "    'validation_data': {'input': test_features, \n",
    "                        'output': test_labels},\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# All lists of parameters will be used to determine the optimal model during cross validation\n",
    "l2_penalty_opts = [0.01, 0.1, 0.5]\n",
    "dropout_rate_opts  = [0, 0.2, 0.5]\n",
    "hidden_layers_opts = []\n",
    "\n",
    "# For me this was the most important choice, since it seemed as though most of the \n",
    "# other parameters are typically used in object recognition\n",
    "# For the number of nodes and hiddend layers I create all possible combinations of single\n",
    "# double hidden layers with a range of 1 to 1000 nodes in each\n",
    "for i in range(1,1000):\n",
    "    hidden_layers_opts.append(tuple((i,)))\n",
    "    for j in range(1,1000):\n",
    "        hidden_layers_opts.append(tuple((i,j)))\n",
    "\n",
    "# set parameters that will be optimized\n",
    "keras_param_options = {\n",
    "    'hidden_layers': hidden_layers_opts,\n",
    "    'dropout_rate': dropout_rate_opts,  \n",
    "    'l2_penalty': l2_penalty_opts,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   32.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   59.8s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  50 | elapsed:  3.9min remaining:   50.8s\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  50 | elapsed:  4.7min remaining:   17.9s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  4.7min finished\n",
      "/Users/connordepalma/anaconda3/envs/testenv/lib/python3.7/site-packages/ipykernel_launcher.py:51: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/connordepalma/anaconda3/envs/testenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 680 samples, validate on 23 samples\n",
      "Epoch 1/100\n",
      "680/680 [==============================] - 3s 4ms/step - loss: 2.0741 - accuracy: 0.6191 - val_loss: 4.1669 - val_accuracy: 0.2174\n",
      "Epoch 2/100\n",
      "680/680 [==============================] - 0s 265us/step - loss: 1.4767 - accuracy: 0.8456 - val_loss: 3.3817 - val_accuracy: 0.2609\n",
      "Epoch 3/100\n",
      "680/680 [==============================] - 0s 254us/step - loss: 1.2960 - accuracy: 0.9015 - val_loss: 3.3760 - val_accuracy: 0.2174\n",
      "Epoch 4/100\n",
      "680/680 [==============================] - 0s 270us/step - loss: 1.1734 - accuracy: 0.9338 - val_loss: 2.8615 - val_accuracy: 0.3043\n",
      "Epoch 5/100\n",
      "680/680 [==============================] - 0s 272us/step - loss: 1.0981 - accuracy: 0.9309 - val_loss: 2.3291 - val_accuracy: 0.3043\n",
      "Epoch 6/100\n",
      "680/680 [==============================] - 0s 268us/step - loss: 0.9966 - accuracy: 0.9603 - val_loss: 1.9740 - val_accuracy: 0.3043\n",
      "Epoch 7/100\n",
      "680/680 [==============================] - 0s 265us/step - loss: 0.9131 - accuracy: 0.9632 - val_loss: 1.7960 - val_accuracy: 0.3478\n",
      "Epoch 8/100\n",
      "680/680 [==============================] - 0s 259us/step - loss: 0.8438 - accuracy: 0.9632 - val_loss: 1.5982 - val_accuracy: 0.6087\n",
      "Epoch 9/100\n",
      "680/680 [==============================] - 0s 259us/step - loss: 0.8085 - accuracy: 0.9632 - val_loss: 1.4314 - val_accuracy: 0.6522\n",
      "Epoch 10/100\n",
      "680/680 [==============================] - 0s 281us/step - loss: 0.7266 - accuracy: 0.9794 - val_loss: 1.2778 - val_accuracy: 0.6522\n",
      "Epoch 11/100\n",
      "680/680 [==============================] - 0s 271us/step - loss: 0.6753 - accuracy: 0.9750 - val_loss: 1.1610 - val_accuracy: 0.7391\n",
      "Epoch 12/100\n",
      "680/680 [==============================] - 0s 267us/step - loss: 0.6219 - accuracy: 0.9824 - val_loss: 1.0444 - val_accuracy: 0.9565\n",
      "Epoch 13/100\n",
      "680/680 [==============================] - 0s 263us/step - loss: 0.5937 - accuracy: 0.9838 - val_loss: 0.9528 - val_accuracy: 0.9565\n",
      "Epoch 14/100\n",
      "680/680 [==============================] - 0s 253us/step - loss: 0.5459 - accuracy: 0.9809 - val_loss: 0.8721 - val_accuracy: 0.9565\n",
      "Epoch 15/100\n",
      "680/680 [==============================] - 0s 274us/step - loss: 0.4944 - accuracy: 0.9941 - val_loss: 0.7921 - val_accuracy: 0.9565\n",
      "Epoch 16/100\n",
      "680/680 [==============================] - 0s 270us/step - loss: 0.4872 - accuracy: 0.9794 - val_loss: 0.7381 - val_accuracy: 0.9565\n",
      "Epoch 17/100\n",
      "680/680 [==============================] - 0s 267us/step - loss: 0.4311 - accuracy: 0.9897 - val_loss: 0.6954 - val_accuracy: 0.9565\n",
      "Epoch 18/100\n",
      "680/680 [==============================] - 0s 269us/step - loss: 0.4080 - accuracy: 0.9912 - val_loss: 0.6417 - val_accuracy: 0.9565\n",
      "Epoch 19/100\n",
      "680/680 [==============================] - 0s 252us/step - loss: 0.3956 - accuracy: 0.9824 - val_loss: 0.5916 - val_accuracy: 0.9565\n",
      "Epoch 20/100\n",
      "680/680 [==============================] - 0s 260us/step - loss: 0.3554 - accuracy: 0.9853 - val_loss: 0.5545 - val_accuracy: 0.9565\n",
      "Epoch 21/100\n",
      "680/680 [==============================] - 0s 277us/step - loss: 0.3412 - accuracy: 0.9897 - val_loss: 0.5187 - val_accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "680/680 [==============================] - 0s 271us/step - loss: 0.3140 - accuracy: 0.9926 - val_loss: 0.4827 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "680/680 [==============================] - 0s 260us/step - loss: 0.2907 - accuracy: 0.9971 - val_loss: 0.4537 - val_accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "680/680 [==============================] - 0s 269us/step - loss: 0.2758 - accuracy: 0.9897 - val_loss: 0.4303 - val_accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "680/680 [==============================] - 0s 264us/step - loss: 0.2647 - accuracy: 0.9941 - val_loss: 0.4090 - val_accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "680/680 [==============================] - 0s 276us/step - loss: 0.2469 - accuracy: 0.9941 - val_loss: 0.3947 - val_accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "680/680 [==============================] - 0s 283us/step - loss: 0.2231 - accuracy: 0.9941 - val_loss: 0.3687 - val_accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "680/680 [==============================] - 0s 271us/step - loss: 0.2106 - accuracy: 0.9985 - val_loss: 0.3585 - val_accuracy: 0.9565\n",
      "Epoch 29/100\n",
      "680/680 [==============================] - 0s 266us/step - loss: 0.2107 - accuracy: 0.9912 - val_loss: 0.3421 - val_accuracy: 0.9565\n",
      "Epoch 30/100\n",
      "680/680 [==============================] - 0s 253us/step - loss: 0.2055 - accuracy: 0.9912 - val_loss: 0.3200 - val_accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "680/680 [==============================] - 0s 273us/step - loss: 0.1974 - accuracy: 0.9912 - val_loss: 0.3057 - val_accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "680/680 [==============================] - 0s 275us/step - loss: 0.1759 - accuracy: 0.9956 - val_loss: 0.2865 - val_accuracy: 1.0000\n",
      "Best score obtained: -0.13653539193666586\n",
      "Parameters:\n",
      "\tl2_penalty: 0.01\n",
      "\thidden_layers: (31, 956)\n",
      "\tdropout_rate: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Create RandomizedSearchCV object containing previously created parameters\n",
    "# based on http://ethen8181.github.io/machine-learning/keras/nn_keras_hyperparameter_tuning.html\n",
    "rs_keras = RandomizedSearchCV( \n",
    "    model_keras, \n",
    "    param_distributions = keras_param_options,\n",
    "    scoring = 'neg_log_loss',\n",
    "    cv=5,\n",
    "    n_jobs = -1,\n",
    "    verbose = 10\n",
    ")\n",
    "\n",
    "# Fit the data\n",
    "# This will run cross validation on the data set and determine the optimal\n",
    "# parameters for a neural network based off the options provided\n",
    "# The optimal model can then be immediately accessed and saved\n",
    "rs_keras.fit(features, labels,\n",
    "             validation_data = (test_features, test_labels),\n",
    "             callbacks=callbacks,\n",
    "             epochs=100,\n",
    "             batch_size=batch_size,\n",
    "             verbose=1)\n",
    "\n",
    "# Output optimal parameters and corresponding accuracy measurement\n",
    "print('Best score obtained: {0}'.format(rs_keras.best_score_))\n",
    "print('Parameters:')\n",
    "for param, value in rs_keras.best_params_.items():\n",
    "    print('\\t{}: {}'.format(param, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 502us/step\n",
      "loss=0.2865, accuracy: 100.0000%\n",
      "dumping architecture and weights to file...\n"
     ]
    }
   ],
   "source": [
    "# We can evaluate the validation data again on the optimal model for easier to read results\n",
    "# based on https://www.pyimagesearch.com/2016/09/26/a-simple-neural-network-with-python-and-keras/\n",
    "(loss, accuracy) = rs_keras.best_estimator_.model.evaluate(test_features, test_labels,\n",
    "                                                           batch_size=5, verbose=1)\n",
    "\n",
    "print(\"loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "# dump the network architecture and weights to file\n",
    "print(\"dumping architecture and weights to file...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_keras.best_estimator_.model.save(\"./Models/model.hdf5\") # Save the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the parameters are already known from previous CV tests, they can be manually put into the base keras\n",
    "# model function and fitted into a model\n",
    "# This should be faster that running cross-validation, although any possible optimization could be missed\n",
    "# if the input data is changed by a significant amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example values based on my results\n",
    "# l2_pen = 0.01\n",
    "# hl = [159, 993]\n",
    "# dr = 0.2\n",
    "# l2_pen,hl,dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_keras_base(hidden_layers=hl,dropout_rate=dr,l2_penalty=l2_pen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://www.pyimagesearch.com/2016/09/26/a-simple-neural-network-with-python-and-keras/\n",
    "# model.fit(features, labels, epochs=100, batch_size=batch_size,\n",
    "#     verbose=1)\n",
    "\n",
    "# # show the accuracy on the testing set\n",
    "# (loss, accuracy) = model.evaluate(test_features, test_labels,\n",
    "#                                   batch_size=5, verbose=1)\n",
    "\n",
    "# print(\"loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy * 100))\n",
    "\n",
    "# model.save(\"./diff_model.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
