{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import dct, idct\n",
    "from random import shuffle as shuf\n",
    "from imutils import paths\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement 2D DCT\n",
    "def dct2(a):\n",
    "    return dct(dct(a.T, norm='ortho').T, norm='ortho')\n",
    "\n",
    "# implement 2D IDCT\n",
    "def idct2(a):\n",
    "    return idct(idct(a.T, norm='ortho').T, norm='ortho') \n",
    "\n",
    "def dct_compression(img):\n",
    "    imf = np.float32(img)\n",
    "    comp = dct2(imf)/255. # Scale to force low values to 0\n",
    "    comp = np.uint8(idct2(comp)*255)\n",
    "    return comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_disp_img(img, title=\"image\"):\n",
    "    while True:\n",
    "        cv2.imshow(title, img)\n",
    "        k = cv2.waitKey(1)\n",
    "\n",
    "        if k & 0xFF == ord('q'):\n",
    "            # q key pressed so quit\n",
    "            print(\"Quitting...\")\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Generate Structure contiaining info on dataset\n",
    "num_training_samples = 10\n",
    "\n",
    "directory = \"./Dataset/\"\n",
    "names = []\n",
    "\n",
    "train_image_pathname = []\n",
    "train_classes = []\n",
    "train_num_samples = []\n",
    "\n",
    "test_image_pathname = []\n",
    "test_classes = []\n",
    "test_num_samples =[]\n",
    "\n",
    "class_num = 0\n",
    "for name in os.listdir(directory):\n",
    "    print(name)\n",
    "    path = os.path.join(directory,name)\n",
    "    if os.path.isdir(path):\n",
    "        if not name.startswith(\".\"):\n",
    "            i = 0\n",
    "            images = os.listdir(path)\n",
    "            shuf(images)\n",
    "            for image in images:\n",
    "                image_path = os.path.join(path,image)\n",
    "\n",
    "                if i < num_training_samples:\n",
    "                    train_image_pathname += [image_path]\n",
    "                    train_classes += [class_num]\n",
    "                    if len(train_num_samples) > class_num:\n",
    "                        train_num_samples[class_num] += 1\n",
    "                    else:\n",
    "                        train_num_samples += [1]\n",
    "\n",
    "                    if i == 0:\n",
    "                        names += [name]\n",
    "\n",
    "                else:\n",
    "                    test_image_pathname += [image_path]\n",
    "                    test_classes += [class_num]\n",
    "\n",
    "                    if len(test_num_samples) > class_num:\n",
    "                        test_num_samples[class_num] += 1\n",
    "                    else:\n",
    "                        test_num_samples += [1]\n",
    "\n",
    "                i += 1\n",
    "            class_num += 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Class Names: \\n\", names)\n",
    "\n",
    "print(\"\\n\\nTraining images: \\n\", train_image_pathname)\n",
    "print(\"\\nClasses: \\n\", train_classes)\n",
    "print(\"\\nNum Samples: \\n\", train_num_samples)\n",
    "\n",
    "print(\"\\n\\nTesting images: \\n\", test_image_pathname)\n",
    "print(\"\\nClasses: \\n\", test_classes)\n",
    "print(\"\\nNum Samples: \\n\", test_num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePaths = list(paths.list_images(\"./Dataset/Train/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Dataset Matrix & Normalize/DCT\n",
    "width = height = 32\n",
    "size = (width * height)\n",
    "\n",
    "image_mat = np.zeros((len(imagePaths), height,width),dtype=np.uint8)\n",
    "\n",
    "for ct, image in enumerate(imagePaths):\n",
    "    img = cv2.imread(image,0)\n",
    "    img = cv2.resize(img, (height,width), interpolation=cv2.INTER_LINEAR)\n",
    "    norm_img = np.zeros((height,width))\n",
    "    norm_img = cv2.normalize(img, norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "    dct_image = dct_compression(img)\n",
    "    image_mat[ct, :, :] = img"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "image_num = 8\n",
    "original = cv2.imread(imagePaths[image_num],0)\n",
    "# cv_disp_img(original)\n",
    "original_scaled = cv2.resize(original, (height,width), interpolation=cv2.INTER_LINEAR)\n",
    "# cv_disp_img(original_scaled)\n",
    "new_image = image_mat[image_num,:,:]\n",
    "# cv_disp_img(new_image)\n",
    "print(np.count_nonzero(original_scaled==0),np.count_nonzero(new_image==0))\n",
    "cv_disp_img(cv2.hconcat((original_scaled,new_image)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quitting...\n"
     ]
    }
   ],
   "source": [
    "# 2DPCA\n",
    "def give_P(eig_vals,quality):\n",
    "    s = np.sum(eig_vals)\n",
    "    thresh = s * quality/100\n",
    "    t = 0\n",
    "    P = 0\n",
    "    while t < thresh:\n",
    "        t += eig_vals[P]\n",
    "        P += 1\n",
    "        \n",
    "    return P\n",
    "        \n",
    "def reduce_dim(images,mean_subtracted):\n",
    "    num = images.shape[0]\n",
    "    height = images.shape[1]\n",
    "    width = images.shape[2]\n",
    "    \n",
    "    g_t = np.zeros((height,height))\n",
    "    h_t = np.zeros((width,width))\n",
    "    \n",
    "    \n",
    "    for i in range(num):\n",
    "        temp_gt = np.dot(mean_subtracted[i].T,mean_subtracted[i])\n",
    "        temp_ht = np.dot(mean_subtracted[i], mean_subtracted[i].T)\n",
    "        \n",
    "        g_t += temp_gt\n",
    "        h_t += temp_ht\n",
    "    \n",
    "    g_t /= num\n",
    "    h_t /= num\n",
    "    \n",
    "    #g_t\n",
    "    g_e_vals, g_e_vec = np.linalg.eig(g_t)\n",
    "    P_1 = give_P(eig_vals=g_e_vals,quality=90)\n",
    "    print(\"P_1: \",P_1)\n",
    "    new_bases_gt = g_e_vec[:,0:P_1]\n",
    "    \n",
    "    #h_t\n",
    "    h_e_vals, h_e_vec = np.linalg.eig(h_t)\n",
    "    P_2 = give_P(eig_vals=h_e_vals,quality=90)\n",
    "    print(\"P_2: \",P_2)\n",
    "    new_bases_ht = h_e_vec[:,0:P_2]\n",
    "    \n",
    "    new_cord_gt = np.dot(images, new_bases_gt)\n",
    "    \n",
    "    new_cord = np.zeros((num, P_2, P_1))\n",
    "    \n",
    "    for i in range(num):\n",
    "        new_cord[i,:,:] = np.dot(new_bases_ht.T, new_cord_gt[i])\n",
    "    \n",
    "    \n",
    "    return (new_cord, new_bases_gt, new_bases_ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_1:  8\n",
      "P_2:  5\n"
     ]
    }
   ],
   "source": [
    "mean_face = np.mean(image_mat,0)\n",
    "images_mean_subtracted = image_mat - mean_face\n",
    "\n",
    "\n",
    "new_cord,new_bases_gt, new_bases_ht = reduce_dim(image_mat,images_mean_subtracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./PP_Data/new_cord.npy\", new_cord)\n",
    "np.save(\"./PP_Data/new_bases_gt.npy\", new_bases_gt)\n",
    "np.save(\"./PP_Data/new_bases_ht.npy\", new_bases_ht)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "new_cord,new_bases_gt, new_bases_ht = reduce_dim(image_mat,images_mean_subtracted)\n",
    "\n",
    "# image_num = 8\n",
    "# original = cv2.imread(train_image_pathname[image_num],0)\n",
    "# cv_disp_img(original)\n",
    "# original_scaled = cv2.resize(original, (height,width), interpolation=cv2.INTER_LINEAR)\n",
    "# cv_disp_img(original_scaled)\n",
    "# new_image = np.uint8(np.dot(new_cord[image_num], new_bases.T))\n",
    "# cv_disp_img(new_image)\n",
    "# new_image.shape\n",
    "# cv_disp_img(cv2.hconcat((original_scaled,new_image)))\n",
    "\n",
    "horiz = []\n",
    "\n",
    "for ct, image in enumerate(imagePaths):\n",
    "    original = cv2.imread(image,0)\n",
    "    original_scaled = cv2.resize(original, (height,width), interpolation=cv2.INTER_LINEAR)\n",
    "    new_image = np.uint8(np.dot(new_bases_ht,np.dot(new_cord[ct], new_bases_gt.T)))\n",
    "\n",
    "\n",
    "\n",
    "    horiz += [cv2.hconcat((original_scaled,new_image))]\n",
    "cv_disp_img(cv2.vconcat(tuple(horiz)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10/64\n",
      "[INFO] processed 20/64\n",
      "[INFO] processed 30/64\n",
      "[INFO] processed 40/64\n",
      "[INFO] processed 50/64\n",
      "[INFO] processed 60/64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Dad',\n",
       " 'Mom',\n",
       " 'Connor',\n",
       " 'Connor',\n",
       " 'Mom',\n",
       " 'Dad',\n",
       " 'Dad',\n",
       " 'Mom',\n",
       " 'Connor',\n",
       " 'Connor',\n",
       " 'Mom',\n",
       " 'Dad',\n",
       " 'Connor',\n",
       " 'Dad',\n",
       " 'Dad',\n",
       " 'Mom',\n",
       " 'Dad',\n",
       " 'Dad',\n",
       " 'Maddie',\n",
       " 'Mom',\n",
       " 'Maddie',\n",
       " 'Maddie',\n",
       " 'Mom',\n",
       " 'Maddie',\n",
       " 'Maddie',\n",
       " 'Maddie',\n",
       " 'Maddie',\n",
       " 'Maddie',\n",
       " 'Connor',\n",
       " 'Maddie',\n",
       " 'Maddie',\n",
       " 'Connor',\n",
       " 'Maddie',\n",
       " 'Maddie',\n",
       " 'Maddie',\n",
       " 'Maddie',\n",
       " 'Mom',\n",
       " 'Connor',\n",
       " 'Connor',\n",
       " 'Mom',\n",
       " 'Mom',\n",
       " 'Connor',\n",
       " 'Connor',\n",
       " 'Mom',\n",
       " 'Mom',\n",
       " 'Maddie',\n",
       " 'Connor',\n",
       " 'Connor',\n",
       " 'Mom',\n",
       " 'Mom',\n",
       " 'Connor',\n",
       " 'Connor',\n",
       " 'Mom',\n",
       " 'Dad',\n",
       " 'Dad',\n",
       " 'Dad',\n",
       " 'Dad',\n",
       " 'Dad',\n",
       " 'Dad',\n",
       " 'Dad',\n",
       " 'Maddie',\n",
       " 'Maddie',\n",
       " 'Maddie',\n",
       " 'Maddie']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def image_to_feature_vector(image, size=(32, 32)):\n",
    "\t# resize the image to a fixed size, then flatten the image into\n",
    "\t# a list of raw pixel intensities\n",
    "\treturn cv2.resize(image, size).flatten()\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "\t# load the image and extract the class label (assuming that our\n",
    "\t# path as the format: /path/to/dataset/{class}.{image_num}.jpg\n",
    "\timage = new_cord[i]\n",
    "\tlabel = imagePath.split(os.path.sep)[-1].split(\".\")[0]\n",
    "\n",
    "\t# construct a feature vector raw pixel intensities, then update\n",
    "\t# the data matrix and labels list\n",
    "\tfeatures = image_to_feature_vector(image)\n",
    "\tdata.append(features)\n",
    "\tlabels.append(label)\n",
    "\n",
    "\t# show an update every 1,000 images\n",
    "\tif i > 0 and i % 10 == 0:\n",
    "\t\tprint(\"[INFO] processed {}/{}\".format(i, len(imagePaths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data) / 255.0\n",
    "labels = np_utils.to_categorical(labels, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./PP_Data/data.npy\", data)\n",
    "np.save(\"./PP_Data/labels.npy\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainData, testData, trainLabels, testLabels) = train_test_split(\n",
    "\tdata, labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connordepalma/anaconda3/envs/CPE646/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(768, input_dim=1024, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 0.6751 - accuracy: 0.7083\n",
      "Epoch 2/100\n",
      "48/48 [==============================] - 0s 224us/step - loss: 0.4693 - accuracy: 0.7500\n",
      "Epoch 3/100\n",
      "48/48 [==============================] - 0s 221us/step - loss: 0.3795 - accuracy: 0.8125\n",
      "Epoch 4/100\n",
      "48/48 [==============================] - 0s 251us/step - loss: 0.3265 - accuracy: 0.8542\n",
      "Epoch 5/100\n",
      "48/48 [==============================] - 0s 226us/step - loss: 0.2893 - accuracy: 0.8698\n",
      "Epoch 6/100\n",
      "48/48 [==============================] - 0s 249us/step - loss: 0.2620 - accuracy: 0.9167\n",
      "Epoch 7/100\n",
      "48/48 [==============================] - 0s 219us/step - loss: 0.2416 - accuracy: 0.9219\n",
      "Epoch 8/100\n",
      "48/48 [==============================] - 0s 237us/step - loss: 0.2259 - accuracy: 0.9219\n",
      "Epoch 9/100\n",
      "48/48 [==============================] - 0s 230us/step - loss: 0.2132 - accuracy: 0.9271\n",
      "Epoch 10/100\n",
      "48/48 [==============================] - 0s 228us/step - loss: 0.2027 - accuracy: 0.9271\n",
      "Epoch 11/100\n",
      "48/48 [==============================] - 0s 235us/step - loss: 0.1938 - accuracy: 0.9427\n",
      "Epoch 12/100\n",
      "48/48 [==============================] - 0s 212us/step - loss: 0.1861 - accuracy: 0.9479\n",
      "Epoch 13/100\n",
      "48/48 [==============================] - 0s 232us/step - loss: 0.1793 - accuracy: 0.9479\n",
      "Epoch 14/100\n",
      "48/48 [==============================] - 0s 224us/step - loss: 0.1733 - accuracy: 0.9479\n",
      "Epoch 15/100\n",
      "48/48 [==============================] - 0s 250us/step - loss: 0.1679 - accuracy: 0.9479\n",
      "Epoch 16/100\n",
      "48/48 [==============================] - 0s 199us/step - loss: 0.1630 - accuracy: 0.9479\n",
      "Epoch 17/100\n",
      "48/48 [==============================] - 0s 225us/step - loss: 0.1586 - accuracy: 0.9531\n",
      "Epoch 18/100\n",
      "48/48 [==============================] - 0s 225us/step - loss: 0.1544 - accuracy: 0.9583\n",
      "Epoch 19/100\n",
      "48/48 [==============================] - 0s 259us/step - loss: 0.1505 - accuracy: 0.9583\n",
      "Epoch 20/100\n",
      "48/48 [==============================] - 0s 213us/step - loss: 0.1470 - accuracy: 0.9583\n",
      "Epoch 21/100\n",
      "48/48 [==============================] - 0s 225us/step - loss: 0.1436 - accuracy: 0.9583\n",
      "Epoch 22/100\n",
      "48/48 [==============================] - 0s 252us/step - loss: 0.1405 - accuracy: 0.9635\n",
      "Epoch 23/100\n",
      "48/48 [==============================] - 0s 268us/step - loss: 0.1376 - accuracy: 0.9635\n",
      "Epoch 24/100\n",
      "48/48 [==============================] - 0s 238us/step - loss: 0.1350 - accuracy: 0.9688\n",
      "Epoch 25/100\n",
      "48/48 [==============================] - 0s 234us/step - loss: 0.1325 - accuracy: 0.9688\n",
      "Epoch 26/100\n",
      "48/48 [==============================] - 0s 245us/step - loss: 0.1302 - accuracy: 0.9740\n",
      "Epoch 27/100\n",
      "48/48 [==============================] - 0s 246us/step - loss: 0.1280 - accuracy: 0.9740\n",
      "Epoch 28/100\n",
      "48/48 [==============================] - 0s 249us/step - loss: 0.1259 - accuracy: 0.9740\n",
      "Epoch 29/100\n",
      "48/48 [==============================] - 0s 238us/step - loss: 0.1239 - accuracy: 0.9792\n",
      "Epoch 30/100\n",
      "48/48 [==============================] - 0s 253us/step - loss: 0.1220 - accuracy: 0.9792\n",
      "Epoch 31/100\n",
      "48/48 [==============================] - 0s 211us/step - loss: 0.1203 - accuracy: 0.9792\n",
      "Epoch 32/100\n",
      "48/48 [==============================] - 0s 228us/step - loss: 0.1186 - accuracy: 0.9792\n",
      "Epoch 33/100\n",
      "48/48 [==============================] - 0s 246us/step - loss: 0.1169 - accuracy: 0.9792\n",
      "Epoch 34/100\n",
      "48/48 [==============================] - 0s 238us/step - loss: 0.1153 - accuracy: 0.9792\n",
      "Epoch 35/100\n",
      "48/48 [==============================] - 0s 219us/step - loss: 0.1138 - accuracy: 0.9792\n",
      "Epoch 36/100\n",
      "48/48 [==============================] - 0s 224us/step - loss: 0.1123 - accuracy: 0.9792\n",
      "Epoch 37/100\n",
      "48/48 [==============================] - 0s 215us/step - loss: 0.1109 - accuracy: 0.9844\n",
      "Epoch 38/100\n",
      "48/48 [==============================] - 0s 234us/step - loss: 0.1094 - accuracy: 0.9844\n",
      "Epoch 39/100\n",
      "48/48 [==============================] - 0s 220us/step - loss: 0.1081 - accuracy: 0.9844\n",
      "Epoch 40/100\n",
      "48/48 [==============================] - 0s 230us/step - loss: 0.1067 - accuracy: 0.9896\n",
      "Epoch 41/100\n",
      "48/48 [==============================] - 0s 251us/step - loss: 0.1055 - accuracy: 0.9896\n",
      "Epoch 42/100\n",
      "48/48 [==============================] - 0s 239us/step - loss: 0.1042 - accuracy: 0.9896\n",
      "Epoch 43/100\n",
      "48/48 [==============================] - 0s 227us/step - loss: 0.1030 - accuracy: 0.9896\n",
      "Epoch 44/100\n",
      "48/48 [==============================] - 0s 216us/step - loss: 0.1019 - accuracy: 0.9896\n",
      "Epoch 45/100\n",
      "48/48 [==============================] - 0s 248us/step - loss: 0.1008 - accuracy: 0.9896\n",
      "Epoch 46/100\n",
      "48/48 [==============================] - 0s 237us/step - loss: 0.0997 - accuracy: 0.9896\n",
      "Epoch 47/100\n",
      "48/48 [==============================] - 0s 261us/step - loss: 0.0986 - accuracy: 0.9896\n",
      "Epoch 48/100\n",
      "48/48 [==============================] - 0s 251us/step - loss: 0.0975 - accuracy: 0.9896\n",
      "Epoch 49/100\n",
      "48/48 [==============================] - 0s 239us/step - loss: 0.0965 - accuracy: 0.9896\n",
      "Epoch 50/100\n",
      "48/48 [==============================] - 0s 257us/step - loss: 0.0956 - accuracy: 0.9896\n",
      "Epoch 51/100\n",
      "48/48 [==============================] - 0s 280us/step - loss: 0.0946 - accuracy: 0.9896\n",
      "Epoch 52/100\n",
      "48/48 [==============================] - 0s 276us/step - loss: 0.0937 - accuracy: 0.9896\n",
      "Epoch 53/100\n",
      "48/48 [==============================] - 0s 255us/step - loss: 0.0927 - accuracy: 0.9948\n",
      "Epoch 54/100\n",
      "48/48 [==============================] - 0s 235us/step - loss: 0.0918 - accuracy: 0.9948\n",
      "Epoch 55/100\n",
      "48/48 [==============================] - 0s 246us/step - loss: 0.0910 - accuracy: 0.9948\n",
      "Epoch 56/100\n",
      "48/48 [==============================] - 0s 233us/step - loss: 0.0901 - accuracy: 0.9948\n",
      "Epoch 57/100\n",
      "48/48 [==============================] - 0s 258us/step - loss: 0.0892 - accuracy: 0.9948\n",
      "Epoch 58/100\n",
      "48/48 [==============================] - 0s 242us/step - loss: 0.0884 - accuracy: 0.9948\n",
      "Epoch 59/100\n",
      "48/48 [==============================] - 0s 274us/step - loss: 0.0874 - accuracy: 0.9948\n",
      "Epoch 60/100\n",
      "48/48 [==============================] - 0s 250us/step - loss: 0.0866 - accuracy: 0.9948\n",
      "Epoch 61/100\n",
      "48/48 [==============================] - 0s 237us/step - loss: 0.0858 - accuracy: 0.9948\n",
      "Epoch 62/100\n",
      "48/48 [==============================] - 0s 272us/step - loss: 0.0851 - accuracy: 0.9948\n",
      "Epoch 63/100\n",
      "48/48 [==============================] - 0s 274us/step - loss: 0.0843 - accuracy: 0.9948\n",
      "Epoch 64/100\n",
      "48/48 [==============================] - 0s 260us/step - loss: 0.0836 - accuracy: 0.9948\n",
      "Epoch 65/100\n",
      "48/48 [==============================] - 0s 245us/step - loss: 0.0828 - accuracy: 0.9948\n",
      "Epoch 66/100\n",
      "48/48 [==============================] - 0s 271us/step - loss: 0.0821 - accuracy: 0.9948\n",
      "Epoch 67/100\n",
      "48/48 [==============================] - 0s 251us/step - loss: 0.0814 - accuracy: 0.9948\n",
      "Epoch 68/100\n",
      "48/48 [==============================] - 0s 242us/step - loss: 0.0807 - accuracy: 0.9948\n",
      "Epoch 69/100\n",
      "48/48 [==============================] - 0s 268us/step - loss: 0.0800 - accuracy: 0.9948\n",
      "Epoch 70/100\n",
      "48/48 [==============================] - 0s 255us/step - loss: 0.0793 - accuracy: 0.9948\n",
      "Epoch 71/100\n",
      "48/48 [==============================] - 0s 239us/step - loss: 0.0787 - accuracy: 0.9948\n",
      "Epoch 72/100\n",
      "48/48 [==============================] - 0s 234us/step - loss: 0.0780 - accuracy: 0.9948\n",
      "Epoch 73/100\n",
      "48/48 [==============================] - 0s 258us/step - loss: 0.0774 - accuracy: 0.9948\n",
      "Epoch 74/100\n",
      "48/48 [==============================] - 0s 239us/step - loss: 0.0767 - accuracy: 0.9948\n",
      "Epoch 75/100\n",
      "48/48 [==============================] - 0s 234us/step - loss: 0.0761 - accuracy: 0.9948\n",
      "Epoch 76/100\n",
      "48/48 [==============================] - 0s 261us/step - loss: 0.0754 - accuracy: 0.9948\n",
      "Epoch 77/100\n",
      "48/48 [==============================] - 0s 245us/step - loss: 0.0748 - accuracy: 0.9948\n",
      "Epoch 78/100\n",
      "48/48 [==============================] - 0s 259us/step - loss: 0.0742 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "48/48 [==============================] - 0s 240us/step - loss: 0.0736 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "48/48 [==============================] - 0s 283us/step - loss: 0.0730 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "48/48 [==============================] - 0s 266us/step - loss: 0.0725 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "48/48 [==============================] - 0s 275us/step - loss: 0.0719 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "48/48 [==============================] - 0s 247us/step - loss: 0.0714 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "48/48 [==============================] - 0s 238us/step - loss: 0.0708 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "48/48 [==============================] - 0s 231us/step - loss: 0.0703 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "48/48 [==============================] - 0s 232us/step - loss: 0.0698 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "48/48 [==============================] - 0s 251us/step - loss: 0.0692 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "48/48 [==============================] - 0s 227us/step - loss: 0.0687 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "48/48 [==============================] - 0s 237us/step - loss: 0.0682 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "48/48 [==============================] - 0s 250us/step - loss: 0.0677 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "48/48 [==============================] - 0s 237us/step - loss: 0.0672 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "48/48 [==============================] - 0s 223us/step - loss: 0.0668 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "48/48 [==============================] - 0s 220us/step - loss: 0.0663 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "48/48 [==============================] - 0s 243us/step - loss: 0.0658 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "48/48 [==============================] - 0s 241us/step - loss: 0.0653 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "48/48 [==============================] - 0s 228us/step - loss: 0.0649 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "48/48 [==============================] - 0s 221us/step - loss: 0.0644 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "48/48 [==============================] - 0s 245us/step - loss: 0.0640 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "48/48 [==============================] - 0s 228us/step - loss: 0.0635 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "48/48 [==============================] - 0s 222us/step - loss: 0.0631 - accuracy: 1.0000\n",
      "[INFO] evaluating on testing set...\n",
      "16/16 [==============================] - 0s 10ms/step\n",
      "[INFO] loss=0.1157, accuracy: 95.3125%\n",
      "[INFO] dumping architecture and weights to file...\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(768, input_dim=1024, init=\"uniform\",\n",
    "\tactivation=\"relu\"))\n",
    "model.add(Dense(384, activation=\"relu\", kernel_initializer=\"uniform\"))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# train the model using SGD\n",
    "print(\"[INFO] compiling model...\")\n",
    "sgd = SGD(lr=0.01)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=sgd,\n",
    "\tmetrics=[\"accuracy\"])\n",
    "model.fit(trainData, trainLabels, epochs=100, batch_size=128,\n",
    "\tverbose=1)\n",
    "\n",
    "# show the accuracy on the testing set\n",
    "print(\"[INFO] evaluating on testing set...\")\n",
    "(loss, accuracy) = model.evaluate(testData, testLabels,\n",
    "\tbatch_size=128, verbose=1)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss,\n",
    "\taccuracy * 100))\n",
    "\n",
    "# dump the network architecture and weights to file\n",
    "print(\"[INFO] dumping architecture and weights to file...\")\n",
    "model.save(\"./model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\"Connor\", \"Maddie\", \"Dad\", \"Mom\"]\n",
    "model = load_model(\"./model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_faces():\n",
    "    cv2.namedWindow(\"aha\")\n",
    "    \n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")    \n",
    "    \n",
    "    webcam = cv2.VideoCapture(0)\n",
    "    \n",
    "    success = webcam.isOpened()\n",
    "    if success == False:\n",
    "        print('Error: Camera could not be opened')\n",
    "    else:\n",
    "        print('Success: Grabbed the camera')\n",
    "    while True:\n",
    "        ret, frame = webcam.read()\n",
    "        label = \"\"\n",
    "        if ret:\n",
    "            faces = face_cascade.detectMultiScale(frame, scaleFactor=1.1, minNeighbors=5) \n",
    "            for (x,y,w,h) in faces:\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 1);\n",
    "\n",
    "                p=10\n",
    "                img = cv2.cvtColor(frame[y-p+1:y+h+p, x-p+1:x+w+p], cv2.COLOR_BGR2GRAY)\n",
    "                img = cv2.resize(img, (32,32), interpolation=cv2.INTER_LINEAR)\n",
    "                norm_img = np.zeros((32,32))\n",
    "                norm_img = cv2.normalize(img, norm_img, 0, 255, cv2.NORM_MINMAX)\n",
    "                dct_image = dct_compression(img)\n",
    "                face = np.dot(new_bases_ht.T,np.dot(dct_image,new_bases_gt))\n",
    "\n",
    "                features = np.array([image_to_feature_vector(face)/255.])\n",
    "                probs = model.predict(features)[0]\n",
    "                prediction = probs.argmax(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "                label = \"{}: {:.2f}%\".format(CLASSES[prediction],\n",
    "                probs[prediction] * 100)\n",
    "                cv2.putText(frame, label, (x, y), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            1.0, (0, 255, 0), 3)\n",
    "            cv2.imshow(\"ahahaha\", frame)\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "        # Monitor keystrokes\n",
    "        k = cv2.waitKey(1)\n",
    "\n",
    "        if k & 0xFF == ord('q'):\n",
    "            print(\"Quitting...\")\n",
    "            break\n",
    "       \n",
    "    webcam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Grabbed the camera\n",
      "Quitting...\n"
     ]
    }
   ],
   "source": [
    "recognize_faces()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CPE646",
   "language": "python",
   "name": "cpe646"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
