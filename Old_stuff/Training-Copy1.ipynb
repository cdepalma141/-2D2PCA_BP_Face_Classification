{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_disp_img(img, title=\"image\"):\n",
    "    while True:\n",
    "        cv2.imshow(title, img)\n",
    "        k = cv2.waitKey(1)\n",
    "\n",
    "        if k & 0xFF == ord('q'):\n",
    "            # q key pressed so quit\n",
    "            print(\"Quitting...\")\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((680, 225), (680, 4), 68)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.load(\"./PP_Data/features.npy\")\n",
    "labels = np.load(\"./PP_Data/labels.npy\")\n",
    "\n",
    "test_features = np.load(\"./PP_Data/test_features.npy\")\n",
    "test_labels = np.load(\"./PP_Data/test_labels.npy\")\n",
    "\n",
    "X_train = features\n",
    "y_train = labels\n",
    "X_val = test_features\n",
    "y_val = test_labels\n",
    "n_input = features.shape[1]\n",
    "n_class = labels.shape[1]\n",
    "batch_size = X_train.shape[0]//10\n",
    "X_train.shape,y_train.shape,batch_size"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cv_disp_img(np.uint8(np.dot(new_bases_ht,np.dot(new_cord[10],new_bases_gt.T))))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(trainData, testData, trainLabels, testLabels) = train_test_split(\n",
    "\tdata, labels, test_size=.25, random_state=42)\n",
    "X_train = trainData\n",
    "y_train = trainLabels\n",
    "X_val = testData\n",
    "y_val = testLabels\n",
    "n_input = trainData.shape[1]\n",
    "n_class = trainLabels.shape[1]\n",
    "X_val.shape,y_val.shape,X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 225)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_class,n_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_keras_base(hidden_layers = [50], dropout_rate = 0, \n",
    "                     l2_penalty = 0.1, optimizer = 'adam',\n",
    "                     n_input = n_input, n_class = n_class):\n",
    "    \"\"\"\n",
    "    Keras Multi-layer neural network. Fixed parameters include: \n",
    "    1. activation function (PRelu)\n",
    "    2. always uses batch normalization after the activation\n",
    "    3. use adam as the optimizer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Tunable parameters are (commonly tuned)\n",
    "    \n",
    "    hidden_layers: list\n",
    "        the number of hidden layers, and the size of each hidden layer\n",
    "    \n",
    "    dropout_rate: float 0 ~ 1\n",
    "        if bigger than 0, there will be a dropout layer\n",
    "    \n",
    "    l2_penalty: float\n",
    "        or so called l2 regularization\n",
    "    \n",
    "    optimizer: string or keras optimizer\n",
    "        method to train the network\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : \n",
    "        a keras model\n",
    "\n",
    "    Reference\n",
    "    ---------\n",
    "    https://keras.io/scikit-learn-api/\n",
    "    \"\"\"   \n",
    "    model = Sequential()   \n",
    "    for index, layers in enumerate(hidden_layers):       \n",
    "        if not index:\n",
    "            # specify the input_dim to be the number of features for the first layer\n",
    "            model.add(Dense(layers, input_dim = n_input, kernel_regularizer = l2(l2_penalty)))\n",
    "        else:\n",
    "            model.add(Dense(layers, kernel_regularizer = l2(l2_penalty)))\n",
    "        \n",
    "        # insert BatchNorm layer immediately after fully connected layers\n",
    "        # and before activation layer\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(PReLU())        \n",
    "        if dropout_rate:\n",
    "            model.add(Dropout(p = dropout_rate))\n",
    "    \n",
    "    model.add(Dense(n_class))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # the loss for binary and muti-class classification is different \n",
    "    loss = 'binary_crossentropy'\n",
    "    if n_class > 2:\n",
    "        loss = 'categorical_crossentropy'\n",
    "    \n",
    "    model.compile(loss = loss, optimizer = optimizer, metrics = ['accuracy'])   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass in fixed parameters n_input and n_class\n",
    "model_keras = KerasClassifier(\n",
    "    build_fn = build_keras_base,\n",
    "    n_input = n_input,\n",
    "    n_class = n_class,\n",
    ")\n",
    "\n",
    "# random search's parameter:\n",
    "# specify the options and store them inside the dictionary\n",
    "# batch size and training method can also be hyperparameters, \n",
    "# but it is fixed\n",
    "early_stop = EarlyStopping(\n",
    "    monitor = 'val_loss', min_delta = 0.1, patience = 5, verbose = 0)\n",
    "\n",
    "callbacks = [early_stop]\n",
    "keras_fit_params = {   \n",
    "    'callbacks': callbacks,\n",
    "    'epochs': 100,\n",
    "    'batch_size': batch_size,\n",
    "    'validation_data': {'input': X_val, \n",
    "                        'output': y_val},\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "dropout_rate_opts  = [0, 0.2, 0.5]\n",
    "hidden_layers_opts = []\n",
    "for i in range(1,800):\n",
    "    hidden_layers_opts.append(tuple((i,)))\n",
    "    for j in range(1,1000):\n",
    "        hidden_layers_opts.append(tuple((i,j)))\n",
    "#         for k in range(1,300):\n",
    "#             hidden_layers_opts.append(tuple((i,j,k)))\n",
    "# hidden_layers_opts = [(, 2, 64, 64), (32, 32, 32, 32, 32), (100, 100, 100)]\n",
    "l2_penalty_opts = [0.01, 0.1, 0.5]\n",
    "keras_param_options = {\n",
    "    'hidden_layers': hidden_layers_opts,\n",
    "    'dropout_rate': dropout_rate_opts,  \n",
    "    'l2_penalty': l2_penalty_opts,\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   43.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done  41 out of  50 | elapsed:  4.4min remaining:   57.7s\n",
      "[Parallel(n_jobs=-1)]: Done  47 out of  50 | elapsed:  4.9min remaining:   18.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  4.9min finished\n",
      "/Users/connordepalma/anaconda3/envs/CPE646/lib/python3.7/site-packages/ipykernel_launcher.py:48: UserWarning: Update your `Dropout` call to the Keras 2 API: `Dropout(rate=0.5)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/connordepalma/anaconda3/envs/CPE646/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 680 samples, validate on 23 samples\n",
      "Epoch 1/100\n",
      "680/680 [==============================] - 3s 4ms/step - loss: 9.9731 - accuracy: 0.6824 - val_loss: 45.4745 - val_accuracy: 0.2174\n",
      "Epoch 2/100\n",
      "680/680 [==============================] - 0s 460us/step - loss: 8.3115 - accuracy: 0.9147 - val_loss: 32.0866 - val_accuracy: 0.2174\n",
      "Epoch 3/100\n",
      "680/680 [==============================] - 0s 494us/step - loss: 7.0348 - accuracy: 0.9691 - val_loss: 22.1564 - val_accuracy: 0.2174\n",
      "Epoch 4/100\n",
      "680/680 [==============================] - 0s 492us/step - loss: 5.8817 - accuracy: 0.9809 - val_loss: 16.9881 - val_accuracy: 0.2174\n",
      "Epoch 5/100\n",
      "680/680 [==============================] - 0s 498us/step - loss: 4.8626 - accuracy: 0.9882 - val_loss: 12.3628 - val_accuracy: 0.2174\n",
      "Epoch 6/100\n",
      "680/680 [==============================] - 0s 477us/step - loss: 3.9669 - accuracy: 0.9941 - val_loss: 8.2684 - val_accuracy: 0.2609\n",
      "Epoch 7/100\n",
      "680/680 [==============================] - 0s 459us/step - loss: 3.2420 - accuracy: 0.9868 - val_loss: 4.8485 - val_accuracy: 0.4783\n",
      "Epoch 8/100\n",
      "680/680 [==============================] - 0s 475us/step - loss: 2.6335 - accuracy: 0.9956 - val_loss: 2.9931 - val_accuracy: 0.6957\n",
      "Epoch 9/100\n",
      "680/680 [==============================] - 0s 463us/step - loss: 2.1438 - accuracy: 0.9985 - val_loss: 2.2562 - val_accuracy: 0.9565\n",
      "Epoch 10/100\n",
      "680/680 [==============================] - 0s 442us/step - loss: 1.7574 - accuracy: 0.9956 - val_loss: 1.7511 - val_accuracy: 0.9565\n",
      "Epoch 11/100\n",
      "680/680 [==============================] - 0s 474us/step - loss: 1.4604 - accuracy: 0.9882 - val_loss: 1.3930 - val_accuracy: 1.0000\n",
      "Epoch 12/100\n",
      "680/680 [==============================] - 0s 466us/step - loss: 1.2176 - accuracy: 0.9897 - val_loss: 1.1711 - val_accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "680/680 [==============================] - 0s 447us/step - loss: 1.0290 - accuracy: 0.9926 - val_loss: 1.0086 - val_accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "680/680 [==============================] - 0s 462us/step - loss: 0.8788 - accuracy: 0.9971 - val_loss: 0.8426 - val_accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "680/680 [==============================] - 0s 479us/step - loss: 0.7757 - accuracy: 0.9912 - val_loss: 0.7311 - val_accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "680/680 [==============================] - 0s 465us/step - loss: 0.6680 - accuracy: 0.9941 - val_loss: 0.6784 - val_accuracy: 0.9565\n",
      "Epoch 17/100\n",
      "680/680 [==============================] - 0s 488us/step - loss: 0.5928 - accuracy: 0.9897 - val_loss: 0.5619 - val_accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "680/680 [==============================] - 0s 521us/step - loss: 0.5145 - accuracy: 0.9956 - val_loss: 0.5157 - val_accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "680/680 [==============================] - 0s 506us/step - loss: 0.4606 - accuracy: 0.9971 - val_loss: 0.4718 - val_accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "680/680 [==============================] - 0s 530us/step - loss: 0.4084 - accuracy: 0.9941 - val_loss: 0.4481 - val_accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "680/680 [==============================] - 0s 531us/step - loss: 0.3796 - accuracy: 0.9912 - val_loss: 0.4399 - val_accuracy: 0.9565\n",
      "Epoch 22/100\n",
      "680/680 [==============================] - 0s 492us/step - loss: 0.3532 - accuracy: 0.9926 - val_loss: 0.3856 - val_accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "680/680 [==============================] - 0s 527us/step - loss: 0.3557 - accuracy: 0.9897 - val_loss: 0.3776 - val_accuracy: 0.9565\n",
      "Epoch 24/100\n",
      "680/680 [==============================] - 0s 524us/step - loss: 0.3346 - accuracy: 0.9956 - val_loss: 0.5108 - val_accuracy: 0.9130\n",
      "Epoch 25/100\n",
      "680/680 [==============================] - 0s 525us/step - loss: 0.3249 - accuracy: 0.9926 - val_loss: 0.3903 - val_accuracy: 0.9565\n",
      "Best score obtained: -0.047509155357918414\n",
      "Parameters:\n",
      "\tl2_penalty: 0.01\n",
      "\thidden_layers: (768, 499)\n",
      "\tdropout_rate: 0.5\n"
     ]
    }
   ],
   "source": [
    "# `verbose` 2 will print the class info for every cross validation, \n",
    "# kind of too much\n",
    "rs_keras = RandomizedSearchCV( \n",
    "    model_keras, \n",
    "    param_distributions = keras_param_options,\n",
    "    scoring = 'neg_log_loss',\n",
    "    cv=5,\n",
    "    n_jobs = -1,\n",
    "    verbose = 10\n",
    ")\n",
    "rs_keras.fit(X_train, y_train,\n",
    "             validation_data = (X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            epochs=100,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1)\n",
    "\n",
    "print('Best score obtained: {0}'.format(rs_keras.best_score_))\n",
    "print('Parameters:')\n",
    "for param, value in rs_keras.best_params_.items():\n",
    "    print('\\t{}: {}'.format(param, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.01, (768, 499), 0.5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_pen = rs_keras.best_params_[\"l2_penalty\"]\n",
    "hl = rs_keras.best_params_[\"hidden_layers\"]\n",
    "dr = rs_keras.best_params_[\"dropout_rate\"]\n",
    "l2_pen,hl,dr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 662us/step\n",
      "[INFO] loss=0.3903, accuracy: 95.6522%\n",
      "[INFO] dumping architecture and weights to file...\n"
     ]
    }
   ],
   "source": [
    "(loss, accuracy) = rs_keras.best_estimator_.model.evaluate(X_val, y_val,\n",
    "    batch_size=5, verbose=1)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss,\n",
    "    accuracy * 100))\n",
    "\n",
    "# dump the network architecture and weights to file\n",
    "print(\"[INFO] dumping architecture and weights to file...\")\n",
    "rs_keras.best_estimator_.model.save(\"./model.hdf5\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = build_keras_base(hidden_layers=hl,dropout_rate=dr,l2_penalty=l2_pen)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=batch_size,\n",
    "    verbose=1)\n",
    "\n",
    "# show the accuracy on the testing set\n",
    "print(\"[INFO] evaluating on testing set...\")\n",
    "(loss, accuracy) = model.evaluate(X_val, y_val,\n",
    "    batch_size=5, verbose=1)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss,\n",
    "    accuracy * 100))\n",
    "\n",
    "# dump the network architecture and weights to file\n",
    "print(\"[INFO] dumping architecture and weights to file...\")\n",
    "model.save(\"./diff_model.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CPE646",
   "language": "python",
   "name": "cpe646"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
